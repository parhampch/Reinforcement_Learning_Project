{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VV8XnOA5dDB"
   },
   "source": [
    "<div align=center>\n",
    "\t\t\n",
    "<p></p>\n",
    "<p></p>\n",
    "<font size=5>\n",
    "In the Name of God\n",
    "<font/>\n",
    "<p></p>\n",
    " <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "<font color=#FF7500>\n",
    "Sharif University of Technology - Departmenet of Computer Engineering\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=blue>\n",
    "Artifical Intelligence - Dr. Mohammad Hossein Rohban\n",
    "</font>\n",
    "<br/>\n",
    "<br/>\n",
    "Fall 2021\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr/>\n",
    "\t\t<div align=center>\n",
    "\t\t    <font color=red size=6>\n",
    "\t\t\t    <br />\n",
    "Practical Assignment 5-Q1\n",
    "            \t<br/>\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    <br/>\n",
    "<font size=4>\n",
    "\t\t\t<br/><br/>\n",
    "Deadline:  Bahman 17th\n",
    "                <br/><b>\n",
    "              Cheating is Strongly Prohibited\n",
    "                </b><br/><br/>\n",
    "                <font color=red>\n",
    "Please run all the cells.\n",
    "     </font>\n",
    "</font>\n",
    "                <br/>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xTcFrqL5dDM"
   },
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collectable": true,
    "execution": {
     "iopub.execute_input": "2021-10-01T16:01:36.762477Z",
     "iopub.status.busy": "2021-10-01T16:01:36.762155Z",
     "iopub.status.idle": "2021-10-01T16:01:36.764025Z",
     "shell.execute_reply": "2021-10-01T16:01:36.763754Z"
    },
    "id": "f3-7dVqZ5dDN"
   },
   "outputs": [],
   "source": [
    "# Set your student number\n",
    "student_number = 98100118\n",
    "Name = 'parham'\n",
    "Last_Name = 'chavoshian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bww0nnn-5dDQ"
   },
   "source": [
    "# Rules\n",
    "- You are not allowed to use provided codes that can be found on the internet. \n",
    "- If you want to use a library which is not already imported, you must ask a question on Quera to get the permission of using that.\n",
    "- Do not hesitate to ask questions on Quera, if you have any.\n",
    "- This assignment is due Bahman 17th 23:59:59. you can use up to 1 grace day for this assignment and the hard deadline is Bahman 18th 23:59:59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WflBoXk5dDT"
   },
   "source": [
    "# Q1 (30Points)\n",
    "\n",
    "## Mountain Car\n",
    "\n",
    "The OpenAI Gym library includes a set of Python Reinforcement Learning environments. In this question, we examine the Mountain Car environment of this collection. Mountain Car is a Reinforcement Learning task that aims to learn the policy of climbing a steep hill and reaching the flag-marked goal. Also, the car engine is not powerful enough to climb straight up the hill on the right. Therefore, it must gain enough acceleration by climbing the hill on the left.\n",
    "\n",
    "\n",
    "In this case, the state of the car is determined by an array containing its position and speed.\n",
    "\n",
    "| Num | Observation | Min | Max|\n",
    "| --- | --- | --- | ---|\n",
    "| 0 | Position | -1.3 | 0.5 |\n",
    "| 1 | Velocity | -0.07 | 0.07|\n",
    "\n",
    "The intelligent agent is allowed to perform three movements of right push, no push and left push in each step. The agent's move will be given to the environment and the environment returns the next state along with the movement reward. For each step that the car does not reach the target, a cost of -1 is considered. Use Q-learning to find the optimal policy in each case. To do this, you must complete the functions below.\n",
    "\n",
    "** Note that you will receive the full score of this question only if you can achieve a score of -115 with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collectable": true,
    "id": "8bf4cK8A5dDW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BRe1Iov16eXa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 15, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "# initialization (parameters such as learning rate, gamma, number of states, number of actions, and q_table)\n",
    "\n",
    "env.reset()\n",
    "pos_low, pos_high = -1.2, 0.6\n",
    "vel_low, vel_high =  -0.07, 0.07\n",
    "num_states = np.array([(0.6 + 1.2) * 10, (0.07 + 0.07) * 100])\n",
    "num_states = np.round(num_states, 0).astype(np.int64) + 1\n",
    "q_table = np.random.uniform(low = -1, high = 1, size = (num_states[0], num_states[1], 3))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cD_eGcgS6iYC"
   },
   "source": [
    "Next cell wants you supplement two functions. First for transforming the continuous space into discrete one (in order to make using q_table feasible), second for updating q_values based on the last action done by agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3b4NwIH96hwG"
   },
   "outputs": [],
   "source": [
    "def discretize_state(x, minn, factor):\n",
    "    return int((x - minn) * factor)\n",
    "\n",
    "\n",
    "def env_state_to_Q_state(state):\n",
    "    [position, velocity] = state\n",
    "    pos_low, pos_high = -1.2, 0.6\n",
    "    vel_low, vel_high =  -0.07, 0.07\n",
    "    disc_state = np.array([discretize_state(position, pos_low, 10), discretize_state(velocity, vel_low, 100)])\n",
    "    return disc_state\n",
    "\n",
    "def update_q(current_state, next_state, action, reward, lr, discount):\n",
    "    temp = lr * (reward + discount * np.max(q_table[next_state[0], next_state[1]]) - q_table[current_state[0], current_state[1], action])\n",
    "    q_table[current_state[0], current_state[1], action] += temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQopme1U6mkQ"
   },
   "source": [
    "At the following cell, the ends of two functions are getting current action based on the policy and defining the training process respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pb46h62R6k0W"
   },
   "outputs": [],
   "source": [
    "def get_action(current_state):\n",
    "    return np.argmax(q_table[current_state[0], current_state[1]])\n",
    "\n",
    "\n",
    "def q_learning():\n",
    "    iter_num = 50000\n",
    "    epsilon = 0.8\n",
    "    lr = 0.1\n",
    "    discount = 0.9\n",
    "    min_epsilon = 0\n",
    "    reduct_epsilon = (epsilon - min_epsilon) / iter_num\n",
    "    reward_list = []\n",
    "    avg_reward_list = []\n",
    "    t = trange(iter_num, desc='', leave=True)\n",
    "    for i in t:\n",
    "        reward, total_reward = 0, 0\n",
    "        curr_state = env.reset()\n",
    "        curr_state = env_state_to_Q_state(curr_state)\n",
    "        done = False\n",
    "        while not(done):\n",
    "            rand_num = np.random.random()\n",
    "            if rand_num < epsilon:\n",
    "                action = np.random.randint(0, 3)\n",
    "            else:\n",
    "                action = get_action(curr_state)\n",
    "            next_state, reward, done, info = env.step(action) \n",
    "            next_state = env_state_to_Q_state(next_state)\n",
    "            if done and next_state[0] >= 0.5:\n",
    "                q_table[next_state[0], next_state[1], action] = reward\n",
    "            else:\n",
    "                update_q(curr_state, next_state, action, reward, lr, discount)\n",
    "            total_reward += reward\n",
    "            curr_state = next_state\n",
    "        if epsilon > min_epsilon:\n",
    "            epsilon -= reduct_epsilon\n",
    "        reward_list.append(total_reward)\n",
    "        if not((i + 1) % 100):\n",
    "            avg_reward_list.append(np.mean(reward_list))\n",
    "            reward_list.clear()\n",
    "            t.set_description(\"Average Reward: {avg}\".format(avg = avg_reward_list[-1]))\n",
    "            t.refresh();\n",
    "\n",
    "\n",
    "def save_policy():\n",
    "    policy  = np.argmax(q_table, axis=2)\n",
    "    np.save('policy.npy', policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "S10pZP4T6tcw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Reward: -200.0: 100%|██████████| 50000/50000 [10:15<00:00, 81.24it/s] \n"
     ]
    }
   ],
   "source": [
    "q_learning()\n",
    "save_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOR1PMec7ube"
   },
   "outputs": [],
   "source": [
    "# Attention: don't change this function. we will use this to grade your policy which you will hand in with policy.npy\n",
    "# btw you can use it to see how you are performing. Uncomment two lines which are commented to be able to see what is happening visually.\n",
    "def score():\n",
    "    policy, scores = np.load(\"policy.npy\"), []\n",
    "    print(policy)\n",
    "    for episode in range(1000):\n",
    "        print(f\"******Episode {episode}\")\n",
    "        state, score, done, step = env_state_to_Q_state(env.reset()), 0, False, 0\n",
    "        while not done:\n",
    "            # time.sleep(0.04)\n",
    "            action = policy[state]\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            state = env_state_to_Q_state(state)\n",
    "            step += 1\n",
    "            score += int(reward)\n",
    "            # env.render()\n",
    "        print(f\"Score:{score}\")\n",
    "        scores.append(score)\n",
    "    print(f\"Average score over 1000 run : {np.array(scores).mean()}\")\n",
    "\n",
    "score()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Q_learning.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "f336af7d54ba0f0c1daaf2256eb85f31e983e88153daf7a27ef3ea6c724faba4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
